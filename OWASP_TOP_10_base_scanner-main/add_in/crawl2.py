from bs4 import BeautifulSoup
from playwright.sync_api import sync_playwright
from urllib.parse import urlparse, urljoin, unquote, parse_qs, urlencode
import json
import time

INPUT_FILE = "/home/ruqos/Desktop/project/etc/crawl_input_type.json"

def read_file():
    with open(f"{INPUT_FILE}", "r") as file:
        return json.load(file)

class FrontCode():
    seen_forms_global = set()  # í´ë˜ìŠ¤ ë‹¨ìœ„ë¡œ ì „ì—­ ì¤‘ë³µ ë°©ì§€

    def __init__(self, path, header, types):
        self.header = header
        self.path = path
        self.login_form = list()
        self.formData = list() # form > action, form > method, input > name, input > valuse, input > type
        self.types= types
    
    def __dict__(self):
        return {
            "path": self.path,
            "header": dict(self.header),  # â† dictë¡œ ë³€í™˜
            "login_form": self.login_form,
            "formData": self.formData,
            "types": self.types            
        }
    
    def get(self, key, default=None):
        return getattr(self, key, default)

    def is_input_field(self, input_tag):
        non_input_types = self.types['non_input_types'] 
        input_types = self.types['input_types']
        
        input_type = input_tag.get('type', 'text').lower()
        if input_type in non_input_types:
            return False
        if input_type not in input_types:
            input_type = 'text'

        if input_tag.has_attr('disabled') or input_tag.has_attr('readonly'):
            return False

        value = input_tag.get('value', '').strip()
        name = input_tag.get('name', '').strip()
        input_id = input_tag.get('id', '').strip()
        likely_placeholder_value = {name, input_id}

        value_condition = value == '' or value in likely_placeholder_value
        has_placeholder = input_tag.has_attr('placeholder')
        autocomplete_off = input_tag.get('autocomplete', '').lower() == 'off'

        return value_condition or has_placeholder or autocomplete_off

    def getFormData(self, html, url=""):
        soup = BeautifulSoup(html, "html.parser")
        forms = soup.find_all("form")

        for form in forms:
            form_action = form.get("action", "")
            form_method = form.get("method", "get").lower()
            inputs = form.find_all("input")
            textarea = form.find_all("textarea")
            input_data = {}

            if form_action == "" or form_action == "#":
                form_action = self.path

            if urlparse(form_action).netloc == '':
                form_action = urljoin(url, form_action)

            # ë¦¬ë‹¤ì´ë ‰ì…˜ ë£¨í”„ê°€ ìˆëŠ” form action í•„í„°ë§
            if 'login?back=' in form_action and form_action.count('login') > 1:
                print(f"[ğŸš«] ë¦¬ë‹¤ì´ë ‰ì…˜ ë£¨í”„ í¼ ì œì™¸: {form_action[:100]}...")
                continue

            passTF = False
            
            for inp in inputs:
                name = inp.get("name")
                value = inp.get("value", "payload")
                type = inp.get("type", "text")
                
                if type == "password":
                    passTF = True
                    
                if name:
                    input_data[name] = "payload" if self.is_input_field(inp) else value
                    
            for inp in textarea:
                name = inp.get("name")
                value = inp.get("value", "payload")
                type = inp.get("type", "text")
                
                if type == "password":
                    passTF = True
                    
                if name:
                    input_data[name] = "payload" if self.is_input_field(inp) else value
                
            if not input_data:
                continue

            form_key = (
                form_action.strip(),
                form_method,
                tuple(sorted(input_data.items()))
            )

            if form_key in FrontCode.seen_forms_global:
                continue
            FrontCode.seen_forms_global.add(form_key)

            form_info = {
                "action": form_action,
                "method": form_method,
                "inputs": input_data,
                "headers": dict(self.header)  # â† dictë¡œ ë³€í™˜
            }
            
            if passTF:
                self.login_form.append(form_info)
            
            self.formData.append(form_info)


visited = set()
page_contents = {}  # URLë³„ HTML ì½˜í…ì¸  ì €ì¥

def normalize_url(url):
    """URLì„ ì •ê·œí™”í•˜ì—¬ ì¤‘ë³µ ë°©ì§€ (back, redirect íŒŒë¼ë¯¸í„° ì œê±°)"""
    parsed = urlparse(url)

    # back íŒŒë¼ë¯¸í„°ê°€ ìˆìœ¼ë©´ ì œê±° (ë¦¬ë‹¤ì´ë ‰ì…˜ ë£¨í”„ ë°©ì§€)
    if parsed.query:
        params = parse_qs(parsed.query)
        # back, redirect ë“± ë¦¬ë‹¤ì´ë ‰ì…˜ ê´€ë ¨ íŒŒë¼ë¯¸í„° ì œê±°
        params.pop('back', None)
        params.pop('redirect', None)
        new_query = urlencode(params, doseq=True)
        normalized = f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
        if new_query:
            normalized += f"?{new_query}"
        return normalized

    return f"{parsed.scheme}://{parsed.netloc}{parsed.path}"

def crawl2(page, url, base_domain, depth=3, start_url=""):

    # URL ì •ê·œí™”
    normalized_url = normalize_url(url)
    clean_url = unquote(normalized_url)

    if depth > 3 or clean_url in visited or len(visited) > 100:
        return

    # ë¡œê·¸ì¸ í˜ì´ì§€ëŠ” í•œ ë²ˆë§Œ í¬ë¡¤ë§
    if 'login' in clean_url.lower() or 'auth' in clean_url.lower() or 'connexion' in clean_url.lower():
        if any('login' in v.lower() or 'auth' in v.lower() or 'connexion' in v.lower() for v in visited):
            print(f"[ğŸš«] ë¡œê·¸ì¸ í˜ì´ì§€ ì¤‘ë³µ ì°¨ë‹¨: {url}")
            return

    visited.add(clean_url)
    print(f"[âœ”] í¬ë¡¤ë§: {url} (ê¹Šì´: {depth})")

    try:
        # í•´ì‹œ(#) URL ì²˜ë¦¬ - SPA ë¼ìš°íŒ…
        if '#' in url:
            # í•´ì‹œê°€ ìˆëŠ” ê²½ìš°, JavaScriptë¡œ ì§ì ‘ navigate
            page.evaluate(f"window.location.hash = '{url.split('#')[1]}'")
            page.wait_for_timeout(500)  # 500msë¡œ ë‹¨ì¶•
            response = None
        else:
            # Playwrightë¡œ í˜ì´ì§€ ë¡œë“œ (networkidle ëŒ€ì‹  domcontentloaded ì‚¬ìš© - í›¨ì”¬ ë¹ ë¦„)
            response = page.goto(url, timeout=5000, wait_until='domcontentloaded')

            if not response:
                print(f"[!] ì‘ë‹µ ì—†ìŒ: {url}")
                return

            # í˜„ì¬ URL í™•ì¸ (ë¦¬ë‹¤ì´ë ‰íŠ¸ ì²´í¬)
            current_url = page.url

            # ë¦¬ë‹¤ì´ë ‰ì…˜ì´ ë°œìƒí–ˆëŠ”ì§€ í™•ì¸
            if current_url != url:
                # ë¡œê·¸ì¸ í˜ì´ì§€ë¡œ ë¦¬ë‹¤ì´ë ‰ì…˜ë˜ì—ˆëŠ”ì§€ í™•ì¸
                if 'login' in current_url.lower() or 'auth' in current_url.lower() or 'connexion' in current_url.lower():
                    print(f"[ğŸš«] ë¡œê·¸ì¸ í•„ìš” í˜ì´ì§€ë¡œ ë¦¬ë‹¤ì´ë ‰ì…˜: {url} â†’ {current_url}")
                    return

                # ì´ë¯¸ ë°©ë¬¸í•œ í˜ì´ì§€ë¡œ ë¦¬ë‹¤ì´ë ‰ì…˜ëœ ê²½ìš°
                normalized_current = normalize_url(current_url)
                if unquote(normalized_current) in visited:
                    print(f"[ğŸš«] ì´ë¯¸ ë°©ë¬¸í•œ í˜ì´ì§€ë¡œ ë¦¬ë‹¤ì´ë ‰ì…˜: {url} â†’ {current_url}")
                    return

            # HTTP ì—ëŸ¬ ìƒíƒœ í™•ì¸
            if response.status >= 400:
                print(f"[!] HTTP ì—ëŸ¬: {url} (ìƒíƒœ: {response.status})")
                return

            # JavaScript ë Œë”ë§ ëŒ€ê¸° (ìµœì†Œí™”)
            page.wait_for_timeout(300)  # 300msë¡œ ë‹¨ì¶• (1500ms â†’ 300ms)

        # í˜„ì¬ URL ì¬í™•ì¸ (JavaScript ì‹¤í–‰ í›„)
        current_url = page.url
        current_domain = urlparse(current_url).netloc

        # localhostì™€ 127.0.0.1ì„ ê°™ì€ ê²ƒìœ¼ë¡œ ì²˜ë¦¬
        def normalize_domain(domain):
            # í¬íŠ¸ í¬í•¨í•˜ì—¬ ì •ê·œí™”
            if domain.startswith('localhost'):
                return domain.replace('localhost', '127.0.0.1')
            return domain

        normalized_current = normalize_domain(current_domain)
        normalized_base = normalize_domain(base_domain)

        # ë‹¤ë¥¸ ë„ë©”ì¸ìœ¼ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸ëœ ê²½ìš° ì¤‘ë‹¨
        if normalized_current != normalized_base:
            print(f"[ğŸš«] ì™¸ë¶€ ë„ë©”ì¸ìœ¼ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸: {current_url}")
            return

        # ë Œë”ë§ëœ HTML ê°€ì ¸ì˜¤ê¸°
        html_content = page.content()
        page_contents[clean_url] = html_content

        soup = BeautifulSoup(html_content, 'html.parser')

        for link in soup.find_all('a'):
            href = link.get('href')
            if not href:
                continue

            # ì™¸ë¶€ ë§í¬ í•„í„°ë§ (localhostì™€ 127.0.0.1ì„ ê°™ì€ ê²ƒìœ¼ë¡œ ì²˜ë¦¬)
            if href.startswith('http'):
                href_domain = urlparse(href).netloc
                normalized_href_domain = normalize_domain(href_domain)
                if normalized_href_domain != normalized_base:
                    continue

            # í•´ì‹œ(#) ë§í¬ ì²˜ë¦¬ - SPA ë¼ìš°íŒ…
            if href.startswith('#/'):
                # ì‹œì‘ URLì— í•´ì‹œ ê²½ë¡œ ì¶”ê°€
                full_url = start_url.split('#')[0] + href
            else:
                full_url = urljoin(url, href)

            parsed_url = urlparse(full_url)

            # ê°™ì€ ë„ë©”ì¸ë§Œ í¬ë¡¤ë§ (localhostì™€ 127.0.0.1 ì •ê·œí™”)
            if parsed_url.netloc != '':
                normalized_link_domain = normalize_domain(parsed_url.netloc)
                if normalized_link_domain != normalized_base:
                    continue

            clean_full_url = unquote(full_url)

            skip_patterns = ['.css', '.js', '.jpg', '.jpeg', '.png', '.gif', '.pdf',
                            '/images/', '/css/', '/js/', '/modules/', '/pdf/', '/redirect']
            if any(pattern in full_url.lower() for pattern in skip_patterns):
                continue

            if (clean_full_url not in visited and
                not ('/login?back=' in full_url and full_url.count('login') > 1)):
                crawl2(page, full_url, base_domain, depth + 1, start_url)

    except Exception as e:
        print(f"[ì—ëŸ¬] {url}: {e}")


def start_crawl2(url, login_path, login_data):
    print("[âœ”] Playwright ê¸°ë°˜ í¬ë¡¤ë§ ì‹œì‘...")
    obj_list = list()

    # visitedì™€ page_contents ì´ˆê¸°í™”
    global visited, page_contents
    visited = set()
    page_contents = {}

    start_url = url
    base_domain = urlparse(start_url).netloc
    print(f"[âœ”] Base Domain: {base_domain}")

    with sync_playwright() as p:
        # Chromium ë¸Œë¼ìš°ì € ì‹¤í–‰ (í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ)
        browser = p.chromium.launch(headless=True)
        context = browser.new_context(
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        )
        page = context.new_page()

        # ë¶ˆí•„ìš”í•œ ë¦¬ì†ŒìŠ¤ ì°¨ë‹¨ìœ¼ë¡œ ì†ë„ í–¥ìƒ (ì´ë¯¸ì§€, í°íŠ¸, ìŠ¤íƒ€ì¼ì‹œíŠ¸ ë“±)
        page.route("**/*", lambda route: route.abort() if route.request.resource_type in ["image", "font", "stylesheet", "media"] else route.continue_())

        # ë¡œê·¸ì¸ ì²˜ë¦¬ (login_dataê°€ ìˆëŠ” ê²½ìš°)
        if login_path and login_data:
            login_url = urljoin(url, login_path)
            print(f"[âœ”] ë¡œê·¸ì¸ ì‹œë„: {login_url}")
            try:
                page.goto(login_url, timeout=5000, wait_until='domcontentloaded')
                page.wait_for_timeout(500)  # 1000ms â†’ 500ms

                # ë¡œê·¸ì¸ í¼ ìë™ ì…ë ¥ (ê°„ë‹¨í•œ êµ¬í˜„)
                for key, value in login_data.items():
                    try:
                        page.fill(f'input[name="{key}"]', value)
                    except:
                        pass

                # ë¡œê·¸ì¸ ë²„íŠ¼ í´ë¦­ ì‹œë„
                try:
                    page.click('button[type="submit"]')
                    page.wait_for_timeout(1000)  # 2000ms â†’ 1000ms
                except:
                    print("[!] ë¡œê·¸ì¸ ë²„íŠ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            except Exception as e:
                print(f"[!] ë¡œê·¸ì¸ ì‹¤íŒ¨: {e}")

        # í¬ë¡¤ë§ ì‹œì‘
        crawl2(page, start_url, base_domain, 0, start_url)

        print(f"\n[âœ”] í¬ë¡¤ë§ ì™„ë£Œëœ ì´ ë§í¬ ìˆ˜: {len(visited)}")

        types = read_file()

        # ì €ì¥ëœ HTML ì½˜í…ì¸ ë¡œ í¼ ë°ì´í„° ì¶”ì¶œ
        for path in visited:
            html_content = page_contents.get(path, "")
            if not html_content:
                continue

            # í—¤ë”ëŠ” ë¹ˆ ë”•ì…”ë„ˆë¦¬ë¡œ ì„¤ì • (Playwrightì—ì„œëŠ” response headers ì ‘ê·¼ì´ ë‹¤ë¦„)
            header = {}
            obj = FrontCode(path, header, types)
            obj.getFormData(html_content, start_url)

            if obj.formData != []:
                obj_list.append(obj)

        browser.close()

    return obj_list



if __name__ == "__main__":
    
    with open("../etc/user_info.json", "r", encoding="utf-8") as f:
        config = json.load(f)
    
    url = config["web_url"]

    obj_list = start_crawl2(url, "", "")

    li = []
    print(f"    â†’ í¬ë¡¤ë§ ì™„ë£Œ: {len(obj_list)}ê°œ í˜ì´ì§€ ë°œê²¬")
    for obj in obj_list:
        dic = obj.__dict__()
        li.append(dic)

    with open("./web_data.json", "w", encoding="utf-8") as f:
        json.dump(li, f, indent=4, ensure_ascii=False)

        # with open("./web_data.json", "w", encoding="utf-8") as f:
        #     json.dump(dic, f, indent=4, ensure_ascii=False)

    # with open("./web_data.json", "w", encoding="utf-8") as f:
    #     # json.dump(li, f, indent=4, ensure_ascii=False)
        
    # with open("./web_data.json", "w", encoding="utf-8") as f:
    #     f.write(a)

        # path = obj.path
        # for formdata in formlist:  #form íƒœê·¸ë³„ ì•ˆì— ìˆëŠ” input íƒœê·¸ì™€ method ì €ì¥ì¥
        #     method = formdata['method']
        #     action = formdata['action']
        #     inputs = formdata['inputs']

            # print(f"[*] URL: {path}\n, Method: {method}, Action: {action}, Inputs: {inputs}")